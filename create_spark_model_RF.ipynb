{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current spark version is 2.4.4\n"
     ]
    }
   ],
   "source": [
    "println(s\"Current spark version is ${spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|800000|\n",
      "|    0|800000|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataSchema = StructType(StructField(target,IntegerType,true), StructField(id,LongType,true), StructField(raw_timestamp,StringType,true), StructField(query_status,StringType,true), StructField(author,StringType,true), StructField(tweet,StringType,true))\n",
       "dataPath = /home/jovyan/data/training.1600000.processed.noemoticon.csv\n",
       "raw_sentiment = [label: int, tweet: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, tweet: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, IntegerType, LongType, StringType}\n",
    "\n",
    "val dataSchema = new StructType()\n",
    "    .add(\"target\", IntegerType)\n",
    "    .add(\"id\", LongType)\n",
    "    .add(\"raw_timestamp\", StringType)\n",
    "    .add(\"query_status\", StringType)\n",
    "    .add(\"author\", StringType)\n",
    "    .add(\"tweet\", StringType)\n",
    "\n",
    "    \n",
    "val dataPath= \"/home/jovyan/data/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "val raw_sentiment = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",false)\n",
    "    .schema(dataSchema)\n",
    "    .load(dataPath)\n",
    "    .selectExpr(\"(case when target=4 then 1 else 0 end) as label\",\"tweet\")\n",
    "\n",
    "raw_sentiment.groupBy($\"label\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed = 5043\n",
       "trainingData = [label: int, tweet: string]\n",
       "testData = [label: int, tweet: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.sp...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, tweet: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "\n",
    "val seed = 5043\n",
    "val Array(trainingData, testData) = raw_sentiment.randomSplit(Array(0.5, 0.5), seed)\n",
    "\n",
    "val tokenizer = new Tokenizer()\n",
    "    .setInputCol(\"tweet\")\n",
    "    .setOutputCol(\"words\")\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "    .setNumFeatures(1000)\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setMaxDepth(3)\n",
    "  .setNumTrees(50)\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "  .setSeed(seed)\n",
    "\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, hashingTF, rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been saved.\n"
     ]
    }
   ],
   "source": [
    "model.write.overwrite().save(\"/home/jovyan/models/spark-ml-model\")\n",
    "println(\"Model has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sameModel = pipeline_678b938ed79d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_678b938ed79d"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sameModel = PipelineModel.load(\"/home/jovyan/models/spark-ml-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|               tweet|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|                 ...|[, , , , , , , , ...|(1000,[107,372],[...|[24.0054049043150...|[0.48010809808630...|       1.0|\n",
      "|    0|                 ...|[, , , , , , , , ...|(1000,[25,109,329...|[25.5729907350614...|[0.51145981470122...|       0.0|\n",
      "|    0|               ju...|[, , , , , , , , ...|(1000,[115,307,32...|[26.6228554042316...|[0.53245710808463...|       0.0|\n",
      "|    0|             i ju...|[, , , , , , , , ...|(1000,[4,307,329,...|[26.6435968852502...|[0.53287193770500...|       0.0|\n",
      "|    0|           FUCK YOU!|[, , , , , , , , ...|(1000,[372,599,82...|[23.8423666437398...|[0.47684733287479...|       1.0|\n",
      "|    0|          i want ...|[, , , , , , , , ...|(1000,[210,329,34...|[26.2903598651949...|[0.52580719730389...|       0.0|\n",
      "|    0|         or i jus...|[, , , , , , , , ...|(1000,[158,187,30...|[26.0903147578361...|[0.52180629515672...|       0.0|\n",
      "|    0|       FS keeps c...|[, , , , , , , fs...|(1000,[58,76,135,...|[25.9404912248759...|[0.51880982449751...|       0.0|\n",
      "|    0|       i really2 ...|[, , , , , , , i,...|(1000,[329,330,37...|[26.7411961431480...|[0.53482392286296...|       0.0|\n",
      "|    0|      My current ...|[, , , , , , my, ...|(1000,[25,82,264,...|[25.5835754531424...|[0.51167150906284...|       0.0|\n",
      "|    0|      this weeken...|[, , , , , , this...|(1000,[368,372,37...|[24.6156402922553...|[0.49231280584510...|       1.0|\n",
      "|    0|     @riceunivers...|[, , , , , @riceu...|(1000,[90,135,230...|[25.0465004918517...|[0.50093000983703...|       0.0|\n",
      "|    0|     I dont like ...|[, , , , , i, don...|(1000,[329,330,35...|[25.9112517807986...|[0.51822503561597...|       0.0|\n",
      "|    0|     I'll get on ...|[, , , , , i'll, ...|(1000,[82,372,378...|[24.0955451951225...|[0.48191090390245...|       1.0|\n",
      "|    0|     sry matt 2 h...|[, , , , , sry, m...|(1000,[372,381,63...|[24.0054049043150...|[0.48010809808630...|       1.0|\n",
      "|    0|     what the fuc...|[, , , , , what, ...|(1000,[372,526,71...|[24.0054049043150...|[0.48010809808630...|       1.0|\n",
      "|    0|     ...lonely night|[, , , , ...lonel...|(1000,[76,372,708...|[24.0054049043150...|[0.48010809808630...|       1.0|\n",
      "|    0|     Bye World!!!!!!|[, , , , bye, wor...|(1000,[305,372,61...|[24.0054049043150...|[0.48010809808630...|       1.0|\n",
      "|    0|    I'll be worki...|[, , , , i'll, be...|(1000,[76,368,372...|[24.0493737365650...|[0.48098747473130...|       1.0|\n",
      "|    0|    My cell phone...|[, , , , my, cell...|(1000,[68,210,266...|[28.3004466077487...|[0.56600893215497...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictionsDF = [label: int, tweet: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, tweet: string ... 5 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsDF = sameModel.transform(testData)\n",
    "\n",
    "predictionsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getProbability = UserDefinedFunction(<function1>,DoubleType,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,DoubleType,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val getProbability = udf((prediction: org.apache.spark.ml.linalg.Vector) => prediction(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|  clean_probability|\n",
      "+-------------------+\n",
      "| 0.5198919019136995|\n",
      "|  0.488540185298772|\n",
      "| 0.4675428919153667|\n",
      "| 0.4671280622949945|\n",
      "| 0.5231526671252036|\n",
      "|0.47419280269610153|\n",
      "|0.47819370484327733|\n",
      "| 0.4811901755024816|\n",
      "| 0.4651760771370384|\n",
      "| 0.4883284909371508|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDF.select(getProbability($\"probability\").alias(\"clean_probability\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
